{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11f2013",
   "metadata": {},
   "source": [
    " Clustering is an unsupervised learning technique, so it is hard to evaluate the quality of the output\n",
    " of any given method. Intuitively, the goal of clustering is to assign points that are similar to the same cluster, and to ensure that points that are dissimilar are in different clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d260efa",
   "metadata": {},
   "source": [
    "There are several ways of measuring clustering. However, measuring metrics may be of limited use. An\n",
    " alternative is to rely on some external form of data with which to validate the method. For example,\n",
    " if we have labels for each object, then we can assume that objects with the same label are similar. We can then use the metrics we discuss below to quantify the quality of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a732d",
   "metadata": {},
   "source": [
    "# Labelled Data - Clustering Quality Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16c021",
   "metadata": {},
   "source": [
    "### Purity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7b374",
   "metadata": {},
   "source": [
    "The purity ranges between 0 (bad) and 1 (good). However, we can trivially achieve a purity of 1 by \n",
    " putting each object into its own cluster, so this measure does not penalize for the number of clusters.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/clusters1.png\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "991d6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Define the clusters\n",
    "clusters = [\n",
    "    {\"A\": 5, \"B\": 1},\n",
    "    {\"A\": 1, \"B\": 4, \"C\": 1},\n",
    "    {\"A\": 2, \"C\": 3}\n",
    "]\n",
    "\n",
    "# Calculate the total points across all clusters\n",
    "total_points = sum(sum(cluster.values()) for cluster in clusters)\n",
    "\n",
    "# Calculate the total points for each majority class count\n",
    "majority_sum = sum(max(cluster.values()) for cluster in clusters)\n",
    "\n",
    "# Calculate and print the purity (purity = majority points / total points)\n",
    "print(f\"Purity: {majority_sum / total_points:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101bf7c",
   "metadata": {},
   "source": [
    "### Rand Index & Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd727f",
   "metadata": {},
   "source": [
    "The Rand Index ranges between 0 (bad) and 1 (good). It measures how similar a predicted clustering is to the true class labels, based on all possible pairs of data points. A score of 1 means that every pair of points was either correctly grouped together or correctly kept apart.\n",
    "\n",
    "However, the Rand Index does not penalize for random chance: even poorly structured clusterings can sometimes get a high score simply by agreeing on many \"different\" pairs. Thatâ€™s why an adjusted version, the Adjusted Rand Index (ARI), is often used as it corrects for the level of agreement expected by chance. \n",
    "\n",
    "The Adjusted Rand Index (ARI) ranges between -1 and 1, where 1 means a perfect match between the predicted clustering and the true labels, 0 means the clustering is no better than random chance, and negative values indicate worse-than-random clusterings.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/clusters1.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c277aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand Index: 0.68\n",
      "Adjusted Rand Index: 0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Define the clusters and their true labels\n",
    "\n",
    "true_labels = ['A','A','A','A','A','B', \n",
    "               'A','B','B','B','B','C', \n",
    "               'A','A','C','C','C']\n",
    "\n",
    "predicted_labels = [0,0,0,0,0,0, \n",
    "                    1,1,1,1,1,1, \n",
    "                    2,2,2,2,2]\n",
    "\n",
    "# Compute Rand Index\n",
    "print(f\"Rand Index: {rand_score(true_labels, predicted_labels):.2f}\")\n",
    "\n",
    "# Compute Adjusted Rand Index\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(true_labels, predicted_labels):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a6fbb",
   "metadata": {},
   "source": [
    "### Mutual Information and Normalized Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31758f00",
   "metadata": {},
   "source": [
    "Mutual Information (MI) measures how much information the predicted clustering shares with the true labels. Higher values mean better agreement, but MI is unnormalized and depends on cluster sizes. Normalized Mutual Information (NMI) scales MI between 0 (no cluster-label ralation) and 1 (perfect correlation).\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/clusters1.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45de1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information: 0.39\n",
      "Normalized Mutual Information: 0.36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score, normalized_mutual_info_score\n",
    "\n",
    "# Define the clusters and their true labels\n",
    "\n",
    "true_labels = ['A','A','A','A','A','B', \n",
    "               'A','B','B','B','B','C', \n",
    "               'A','A','C','C','C']\n",
    "\n",
    "predicted_labels = [0,0,0,0,0,0, \n",
    "                    1,1,1,1,1,1, \n",
    "                    2,2,2,2,2]\n",
    "\n",
    "# Compute Mutual Information\n",
    "print(f\"Mutual Information: {mutual_info_score(true_labels, predicted_labels):.2f}\")\n",
    "\n",
    "# Compute Normalized Mutual Information (NMI)\n",
    "print(f\"Normalized Mutual Information: {normalized_mutual_info_score(true_labels, predicted_labels):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
